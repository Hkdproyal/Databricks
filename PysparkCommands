## How to read the CSV file 
---------------------------------
df = spark.read.format("csv") \
    .option("Header", True) \
    .option("Inferschema", True) \
    .option("sep", ",") \
    .load("/FileStore/tables/snakes_count_100.csv")
display(df)

## Another way to read the CSV file 
-----------------------------------
df1 = spark.read.format("csv") \
      .options(header = "True", inferschema ="True", sep =",") \
      .load("dbfs:/FileStore/tables/snakes_count_10000.csv")
display(df1.count())

## Reading Multiple files []
----------------------------
dfx = spark.read.format("csv") \
      .options(header = "True", inferschema = "True", sep =",") \
      .load(["/FileStore/tables/snakes_count_100.csv","dbfs:/FileStore/tables/snakes_count_10000.csv"])
      
## Reading all the files under the folder
-----------------------------------------
dfc = spark.read.format("csv") \
      .options(header = "True", inferschema = "True", sep = ",") \
      .load("dbfs:/FileStore/tables")
display(dfc)
print(dfc.count())

dfc.printSchema() --> show the structure 

## How to Create Own schema in Pyspark
-----------------------------------------
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

schema_Field = StructType([StructField('id', IntegerType(), True),
                             StructField('name', StringType(), True),
                             StructField('nationality', StringType(), True),
                             StructField('club_team', StringType(), True),
                             StructField('wage_euro', IntegerType(), True)
                             ])
                             
dft = spark.read.format("csv").schema(schema_Field).option("header", True).load('dbfs:/FileStore/tables/')
display(dft)

## alternative way to create Own Schema
--------------------------------------
alternative_schema = ' id Integer, name string, nationality string, club_team string, wage_euro Int'

dff = spark.read.format("csv").schema(alternative_schema).option("header", True).load('dbfs:/FileStore/tables/')
display(dff)


## Filter Conditions:
--------------------
• filter(df.column != 50)
• filter((f.col'column1') > 50) & (f.col'column2') > 50))
• filter((f.col('column1*) > 50)|(f.col('column2') > 50))
• filter(df.column.isNull))
• filter(df.column.isNotNulI())
• filter(df.column.like('%%* ))
• filter(df.name.isin())
• filter(df.column.contains('' ))
• filter(df.column.startswith(''))
• filter(df.column.endswith(''))

##Creating a Dataframe 
-----------------------
employee_data =[(10, "Raj kumar", "1999", "100", "M", 2000),
                (11, "suresh", "1989", "200", "M", 2100),
                (12, "pavan", "1899", "1001", "M", 2700),
                (14, "rukesh", "1699", "101", "f", 2500),
                (15, "jeevan", "1979", "102", "M", 2500)]
employee_schema = ["employee_id", "name", "doj", "employee_dept_id", "gender", "salary" ]

employee_df =spark.createDataFrame(data=employee_data, schema = employee_schema)
display(employee_df)


## Adding column with values 
------------------------------
from pyspark.sql.functions import lit 

example1:
empdf = fg.withColumn("Location", lit("Mumbai")).show()


example2:
tiger = employee_df1.withColumn("Location", lit("Mumbai")).show()

+-----------+---------+----+----------------+------+------+--------+
|employee_id|     name| doj|employee_dept_id|gender|salary|Location|
+-----------+---------+----+----------------+------+------+--------+
|         10|Raj kumar|1999|             100|     M|  2000|  Mumbai|
|         11|   suresh|1989|             200|     M|  2100|  Mumbai|
|         12|    pavan|1899|            1001|     M|  2700|  Mumbai|
|         14|   rukesh|1699|             101|     f|  2500|  Mumbai|
|         15|   jeevan|1979|             102|     M|  2500|  Mumbai|
+-----------+---------+----+----------------+------+------+--------+

## Adding new column with calculation 
----------------------------------------
example1:
from pyspark.sql.functions import concat
Calculation = employee_df1.withColumn("Bonus", employee_df1.salary*0.1).show()

+-----------+---------+----+----------------+------+------+-----+
|employee_id|     name| doj|employee_dept_id|gender|salary|Bonus|
+-----------+---------+----+----------------+------+------+-----+
|         10|Raj kumar|1999|             100|     M|  2000|200.0|
|         11|   suresh|1989|             200|     M|  2100|210.0|
|         12|    pavan|1899|            1001|     M|  2700|270.0|
|         14|   rukesh|1699|             101|     f|  2500|250.0|
|         15|   jeevan|1979|             102|     M|  2500|250.0|
+-----------+---------+----+----------------+------+------+-----+

example2:
from pyspark.sql.functions import concat 
calculations1 = Calculation.withColumn("Base salary", Calculation.Bonus*(25%100)).show()

+-----------+---------+----+----------------+------+------+-----+-----------+
|employee_id|     name| doj|employee_dept_id|gender|salary|Bonus|Base salary|
+-----------+---------+----+----------------+------+------+-----+-----------+
|         10|Raj kumar|1999|             100|     M|  2000|200.0|     5000.0|
|         11|   suresh|1989|             200|     M|  2100|210.0|     5250.0|
|         12|    pavan|1899|            1001|     M|  2700|270.0|     6750.0|
|         14|   rukesh|1699|             101|     f|  2500|250.0|     6250.0|
|         15|   jeevan|1979|             102|     M|  2500|250.0|     6250.0|
+-----------+---------+----+----------------+------+------+-----+-----------+

### Adding two or more Columns at a time 
----------------------------------------
from pyspark.sql.functions import concat 
addingmorecolumns = calculations1.withColumn("example salary", lit(5000)).withColumn("Name_with_id", concat("name","employee_id")).show()

+-----------+---------+----+----------------+------+------+-----+-----------+--------------+------------+
|employee_id|     name| doj|employee_dept_id|gender|salary|Bonus|Base salary|example salary|Name_with_id|
+-----------+---------+----+----------------+------+------+-----+-----------+--------------+------------+
|         10|Raj kumar|1999|             100|     M|  2000|200.0|     5000.0|          5000| Raj kumar10|
|         11|   suresh|1989|             200|     M|  2100|210.0|     5250.0|          5000|    suresh11|
|         12|    pavan|1899|            1001|     M|  2700|270.0|     6750.0|          5000|     pavan12|
|         14|   rukesh|1699|             101|     f|  2500|250.0|     6250.0|          5000|    rukesh14|
|         15|   jeevan|1979|             102|     M|  2500|250.0|     6250.0|          5000|    jeevan15|
+-----------+---------+----+----------------+------+------+-----+-----------+--------------+------------+

## adding space between the name and id 
----------------------------------------
addingmorecolumns = calculations1.withColumn("Name_with_id", lit("_"),"employee_id").show()

+-----------+---------+----+----------------+------+------+-----+-----------+------------+
|employee_id|     name| doj|employee_dept_id|gender|salary|Bonus|Base salary|Name_with_id|
+-----------+---------+----+----------------+------+------+-----+-----------+------------+
|         10|Raj kumar|1999|             100|     M|  2000|200.0|     5000.0|Raj kumar_10|
|         11|   suresh|1989|             200|     M|  2100|210.0|     5250.0|   suresh_11|
|         12|    pavan|1899|            1001|     M|  2700|270.0|     6750.0|    pavan_12|
|         14|   rukesh|1699|             101|     f|  2500|250.0|     6250.0|   rukesh_14|
|         15|   jeevan|1979|             102|     M|  2500|250.0|     6250.0|   jeevan_15|
+-----------+---------+----+----------------+------+------+-----+-----------+------------+

## Renaming the Column Name:
-------------------------------
example1:
Rename = addingspace.withColumnRenamed("Name_with_id","Name_ID")
display(Rename)

+-----------+---------+----+----------------+------+------+-----+-----------+------------+
|employee_id|     name| doj|employee_dept_id|gender|salary|Bonus|Base salary|     Name_ID|
+-----------+---------+----+----------------+------+------+-----+-----------+------------+
|         10|Raj kumar|1999|             100|     M|  2000|200.0|     5000.0|Raj kumar_10|
|         11|   suresh|1989|             200|     M|  2100|210.0|     5250.0|   suresh_11|
|         12|    pavan|1899|            1001|     M|  2700|270.0|     6750.0|    pavan_12|
|         14|   rukesh|1699|             101|     f|  2500|250.0|     6250.0|   rukesh_14|
|         15|   jeevan|1979|             102|     M|  2500|250.0|     6250.0|   jeevan_15|
+-----------+---------+----+----------------+------+------+-----+-----------+------------+

example2:
Rename = addingspace.withColumnRenamed("Name_with_id","Name_ID").withColumnRenamed("doj","Date_of_Joining").show()
+-----------+---------+---------------+----------------+------+------+-----+-----------+------------+
|employee_id|     name|Date_of_Joining|employee_dept_id|gender|salary|Bonus|Base salary|     Name_ID|
+-----------+---------+---------------+----------------+------+------+-----+-----------+------------+
|         10|Raj kumar|           1999|             100|     M|  2000|200.0|     5000.0|Raj kumar_10|
|         11|   suresh|           1989|             200|     M|  2100|210.0|     5250.0|   suresh_11|
|         12|    pavan|           1899|            1001|     M|  2700|270.0|     6750.0|    pavan_12|
|         14|   rukesh|           1699|             101|     f|  2500|250.0|     6250.0|   rukesh_14|
|         15|   jeevan|           1979|             102|     M|  2500|250.0|     6250.0|   jeevan_15|
+-----------+---------+---------------+----------------+------+------+-----+-----------+------------+

## Drop a Column 
----------------
drop = Rename.drop("Name_ID").drop("Date_of_Joining").show()

+-----------+---------+----------------+------+------+-----+-----------+
|employee_id|     name|employee_dept_id|gender|salary|Bonus|Base salary|
+-----------+---------+----------------+------+------+-----+-----------+
|         10|Raj kumar|             100|     M|  2000|200.0|     5000.0|
|         11|   suresh|             200|     M|  2100|210.0|     5250.0|
|         12|    pavan|            1001|     M|  2700|270.0|     6750.0|
|         14|   rukesh|             101|     f|  2500|250.0|     6250.0|
|         15|   jeevan|             102|     M|  2500|250.0|     6250.0|
+-----------+---------+----------------+------+------+-----+-----------+

## JOINS:
---------
## Inner Join
-------------
inner = employee_dfx.join(department_details, employee_dfx.employee_dept_id == department_details.department_id, "inner" )
display(inner)

+-----------+---------+----+----------------+------+------+---------------+-------------+
|employee_id|     name| doj|employee_dept_id|gender|salary|department_name|department_id|
+-----------+---------+----+----------------+------+------+---------------+-------------+
|         10|Raj kumar|1999|             100|     M|  2000|             HR|          100|
|         11|   suresh|1989|             200|     M|  2100|         Supply|          200|
+-----------+---------+----+----------------+------+------+---------------+-------------+



## Full-Outer Join --> Gives the result of common tables + left join tables + right join tables 
------------------------------------------------------------------------------------------------
Outer_join = employee_dfx.join(department_details, employee_dfx.employee_dept_id == department_details.department_id, "full")
display(Outer_join)

+-----------+---------+----+----------------+------+------+---------------+-------------+
|employee_id|     name| doj|employee_dept_id|gender|salary|department_name|department_id|
+-----------+---------+----+----------------+------+------+---------------+-------------+
|         10|Raj kumar|1999|             100|     M|  2000|             HR|          100|
|         14|   rukesh|1699|             101|     f|  2500|           null|         null|
|         15|   jeevan|1979|             102|     M|  2500|           null|         null|
|         11|   suresh|1989|             200|     M|  2100|         Supply|          200|
|       null|     null|null|            null|  null|  null|          Sales|          300|
|       null|     null|null|            null|  null|  null|          Stock|          400|
|         12|    pavan|1899|            1001|     M|  2700|           null|         null|

## Left outer join 
---------------------
Outer_join = employee_dfx.join(department_details, employee_dfx.employee_dept_id == department_details.department_id, "Left").show()

+-----------+---------+----+----------------+------+------+---------------+-------------+
|employee_id|     name| doj|employee_dept_id|gender|salary|department_name|department_id|
+-----------+---------+----+----------------+------+------+---------------+-------------+
|         10|Raj kumar|1999|             100|     M|  2000|             HR|          100|
|         11|   suresh|1989|             200|     M|  2100|         Supply|          200|
|         12|    pavan|1899|            1001|     M|  2700|           null|         null|
|         14|   rukesh|1699|             101|     f|  2500|           null|         null|
|         15|   jeevan|1979|             102|     M|  2500|           null|         null|
+-----------+---------+----+----------------+------+------+---------------+-------------+

## Right outer join 
--------------------
Outer_join = employee_dfx.join(department_details, employee_dfx.employee_dept_id == department_details.department_id, "Right").show()

+-----------+---------+----+----------------+------+------+---------------+-------------+
|employee_id|     name| doj|employee_dept_id|gender|salary|department_name|department_id|
+-----------+---------+----+----------------+------+------+---------------+-------------+
|         10|Raj kumar|1999|             100|     M|  2000|             HR|          100|
|         11|   suresh|1989|             200|     M|  2100|         Supply|          200|
|       null|     null|null|            null|  null|  null|          Sales|          300|
|       null|     null|null|            null|  null|  null|          Stock|          400|
+-----------+---------+----+----------------+------+------+---------------+-------------+


## Left Semi join --> similar to inner join, eliminates the right results 
-------------------------------------------------------------------------
Left_semi_join = employee_dfx.join(department_details, employee_dfx.employee_dept_id == department_details.department_id, "leftsemi").show()

+-----------+---------+----+----------------+------+------+
|employee_id|     name| doj|employee_dept_id|gender|salary|
+-----------+---------+----+----------------+------+------+
|         10|Raj kumar|1999|             100|     M|  2000|
|         11|   suresh|1989|             200|     M|  2100|
+-----------+---------+----+----------------+------+------+


## Left anti join --> eliminates the commom records and gives the o/p
---------------------------------------------------------------------- 
Left_semi_join = employee_dfx.join(department_details, employee_dfx.employee_dept_id == department_details.department_id, "anti").show()

+-----------+------+----+----------------+------+------+
|employee_id|  name| doj|employee_dept_id|gender|salary|
+-----------+------+----+----------------+------+------+
|         12| pavan|1899|            1001|     M|  2700|
|         14|rukesh|1699|             101|     f|  2500|
|         15|jeevan|1979|             102|     M|  2500|
+-----------+------+----+----------------+------+------+

## DBUTILS :
--------
dbutils.fs.help()
dbutils.notebook.help()
dbutils.widgets.help()
----------------------
dbutils.widgets provides utilities for working with notebook widgets. You can create different types of widgets and get their bound value. For more info about a method, use dbutils.widgets.help("methodName").
       
combobox(name: String, defaultValue: String, choices: Seq, label: String): void -> Creates a combobox input widget with a given name, default value and choices
      
dropdown(name: String, defaultValue: String, choices: Seq, label: String): void -> Creates a dropdown input widget a with given name, default value and choices

get(name: String): String -> Retrieves current value of an input widget

getArgument(name: String, optional: String): String -> (DEPRECATED) Equivalent to get

multiselect(name: String, defaultValue: String, choices: Seq, label: String): void -> Creates a multiselect input widget with a given name, default value and choices

remove(name: String): void -> Removes an input widget from the notebook

removeAll: void -> Removes all widgets in the notebook

text(name: String, defaultValue: String, label: String): void -> Creates a text input widget with a given name and default value

dbutils.secrets.help("get")
dbutils.fs.help("cp")
dbutils.notebook.help("exit")


#DBUTILS COMMANDS 
------------------
##list filesystem
-----------------
dbutils.fs.ls("dbfs:/FileStore")
        Out[10]: [FileInfo(path='dbfs:/FileStore/shared_uploads/', name='shared_uploads/', size=0, modificationTime=0),
        FileInfo(path='dbfs:/FileStore/tables/', name='tables/', size=0, modificationTime=0),
        FileInfo(path='dbfs:/FileStore/tables_output/', name='tables_output/', size=0, modificationTime=0
##Head 
-------
dbutils.fs.head("/FileStore/tables/snakes_count_100.csv")
        Out[11]: '"Game Number", "Game Length"\n1, 30\n2, 29\n3, 31\n4, 16\n5, 24\n6, 29\n7, 28\n8, 117\n9, 42\n10, 23\n11, 40\n12, 15\n13, 18\n14, 51\n15, 15\n16, 19\n17, 30\n18, 25\n19, 17\n20, 55\n21, 20\n22, 12\n23, 39\n24, 25\n25, 56\n26, 61\n27, 77\n28, 34\n29, 14\n30, 8\n31, 31\n32, 34\n33, 22\n34, 12\n35, 52\n36, 50\n37, 24\n38, 20\n39, 91\n40, 33\n41, 27\n42, 25\n43, 83\n44, 12\n45, 21\n46, 38\n47, 20\n48, 24\n49, 37\n50, 18\n51, 56\n52, 20\n53, 47\n54, 51\n55, 22\n56, 46\n57, 24\n58, 35\n59, 28\n60, 12\n61, 28\n62, 43\n63, 50\n64, 37\n65, 127\n66, 11\n67, 12\n68, 22\n69, 25\n70, 57\n71, 53\n72, 38\n73, 33\n74, 16\n75, 35\n76, 29\n77, 23\n78, 21\n79, 11\n80, 31\n81, 12\n82, 37\n83, 40\n84, 29\n85, 14\n86, 15\n87, 38\n88, 74\n89, 19\n90, 121\n91, 52\n92, 25\n93, 14\n94, 12\n95, 31\n96, 25\n97, 16\n98, 19\n99, 54\n100, 24\n'

##Creating new folder 
---------------------
dbutils.fs.mkdirs("dbfs:/FileStore/tables/filesystem")

##Copy file to folder 
---------------------
dbutils.fs.cp("/FileStore/tables/snakes_count_100.csv","dbfs:/FileStore/tables/filesystem")

##move command
--------------
dbutils.fs.mv("/FileStore/tables/snakes_count_1000.csv","dbfs:/FileStore/tables/filesystem")

## Create a new file with content 
---------------------------------
dbutils.fs.put("/FileStore/tables/happy.txt", "happy days in future")

## how to see content of a file 
----------------------------
dbutils.fs.head("/FileStore/tables/happy.txt")

## remove a file 
----------------
dbutils.fs.rm("/FileStore/tables/happy.txt")

===============================
## DBUTILS NOTEBOOK COMMANDS |
===============================
## how to run another notebook --> works only in databricks full subscription
-----------------------------
dbutils.notebook.run("another notebook name", timeout in seconds)

## Creating dropdown box 
-------------------------
dbutils.widgets.dropdown("Dropdown","1", [str(x) for x in range(1, 11)])

## Creating combo box  --> there is no any difference between combo and dropdown box, we can only select one option
---------------------
dbutils.widgets.combobox("combobox","1", [str(x) for x in range(1, 11)])

##Multiselect option in widgets 
------------------------------
dbutils.widgets.multiselect("multiselect","car",("car","maruti","benz","volvo"))

#removing the widgets
----------------------
dbutils.widgets.remove("combobox")
dbutils.widgets.removeAll() --> removes all the widgets 



# Pyspark Explode Functions
---------------------------
# Create dataframe with array column 
------------------------------------
array_appliance = [
                   ('Raja',['Tv', 'Refrigirator','Oven', 'AC']),
                   ('Homesh', ['AC','Washingmachine',None]),
                   ('Ram',['Grinder','TV']),
                   ('Ramesh',['Refrigerator','TV',None]),
                   ('Rajesh',None)
                   ]
df_app = spark.createDataFrame(data = array_appliance, schema = ['name','Applainces'])
df_app.printSchema()
display(df_app)

    root
 |-- name: string (nullable = true)
 |-- Applainces: array (nullable = true)
 |    |-- element: string (containsNull = true)
 
 
#Explode array field #Explode array field 
-----------------------------------------

from pyspark.sql.functions import explode

df2 = df_app.select(df_app.name, explode(df_app.Applainces))

df_app.printSchema()
display(df_app)

df2.printSchema()
display(df2)

#explode outer --> This even will consider the NULL values 
----------------------------------------------------------
from pyspark.sql.functions import explode_outer

display(df_app.select(df_app.name, explode_outer(df_app.Applainces)))

#positional explode ---> displays the index numbers of the values
------------------------------------------------------------------
from pyspark.sql.functions import posexplode

display(df_app.select(df_app.name, posexplode(df_app.Applainces)))


#Positional explode with NULL ---> when the values has null values then the index also shows as null
-------------------------------
from pyspark.sql.functions import posexplode_outer
display(df_app.select(df_app.name, posexplode_outer(df_app.Applainces)))


## CASE FUNCTIONS:
-------------------#Sample DatFrame  
data_student = [("Raja","Science",80,"p",90),
                ("Rakesh","Maths",90,"p",70),
                ("Rama","English",20,"F",80),
                ("Rajesh","Science",45,"F",75),
                ("Ramesh","Maths",30,"F",50),
                ("Raghav","Maths",None,"NA",70)]
schema = ["name","Subject","Mark","status", "attendence"]
df = spark.createDataFrame(data=data_student, schema = schema)
df.printSchema()
display(df)

# CASE CONDITION, WHEN.OTHERWISE
---------------------------------
# Method 1 by using When condition:
---------------------------------
from pyspark.sql.functions import when 
dft = df.withColumn("Status", when(df.Mark >= 50, "Pass"). when(df.Mark <50,"Fail").otherwise("Absentee")).show()

(3) Spark Jobs
+------+-------+----+--------+----------+
|  name|Subject|Mark|  Status|attendence|
+------+-------+----+--------+----------+
|  Raja|Science|  80|    Pass|        90|
|Rakesh|  Maths|  90|    Pass|        70|
|  Rama|English|  20|    Fail|        80|
|Rajesh|Science|  45|    Fail|        75|
|Ramesh|  Maths|  30|    Fail|        50|
|Raghav|  Maths|null|Absentee|        70|
+------+-------+----+--------+----------+
 
# Method 2 by using expr function:
-----------------------------------
from pyspark.sql.functions import expr 
tgrx = tgr.withColumn("new_status", expr("CASE WHEN Mark >= 50 THEN 'PASS'"+ "WHEN Mark < 50 THEN 'FAIL'" + "ELSE 'Absence' END")).show()

+----+-------+----+--------+----------+----------+
|name|Subject|Mark|  status|attendence|new_status|
+----+-------+----+--------+----------+----------+
|   R|Science|  80|    pass|        90|      PASS|
|  Ra|  Maths|  90|    pass|        70|      PASS|
| Ram|English|  20|    Fail|        80|      FAIL|
| Raj|Science|  45|    Fail|        75|      FAIL|
|Rame|  Maths|  30|    Fail|        50|      FAIL|
|Ragh|  Maths|null|Absentee|        70|   Absence|
+----+-------+----+--------+----------+----------+

## Multiple Conditons 
---------------------
from pyspark.sql.functions import when 
df4 = tgrx.withColumn("Grade", when((tgrx.Mark >= 80) & (tgrx.attendence >=80), "Distinction") \
                    .when((tgrx.Mark >=50) & (tgrx.attendence >=50), "Good") \
                    .otherwise("Average")).show()
                    
+----+-------+----+--------+----------+----------+-----------+
|name|Subject|Mark|  status|attendence|new_status|      Grade|
+----+-------+----+--------+----------+----------+-----------+
|   R|Science|  80|    pass|        90|      PASS|Distinction|
|  Ra|  Maths|  90|    pass|        70|      PASS|       Good|
| Ram|English|  20|    Fail|        80|      FAIL|    Average|
| Raj|Science|  45|    Fail|        75|      FAIL|    Average|
|Rame|  Maths|  30|    Fail|        50|      FAIL|    Average|
|Ragh|  Maths|null|Absentee|        70|   Absence|    Average|
+----+-------+----+--------+----------+----------+-----------+

## How to find the SPARK VERSION 
---------------------------------
from pyspark.sql import SparkSession 

spark = SparkSession.builder.master("local").getOrCreate()
display(spark.sparkContext.version)

## UNION & UNIONALL
-------------------
## UNION fUNCTIONS:

example--
# Create DataFrame:
employee_data =[(100,"Stephen","1999","100","M",2000),
                 (200,"Philips","2002","200","M",8000),
                 (300,"John","2010","100","",6000)
               ]
employee_schema = ["employee_id","name","doj","employee_dept_id","gender","salary"]
df1 = spark.createDataFrame(data=employee_data, schema = employee_schema)
display(df1)

+-----------+-------+----+----------------+------+------+
|employee_id|   name| doj|employee_dept_id|gender|salary|
+-----------+-------+----+----------------+------+------+
|        100|Stephen|1999|             100|     M|  2000|
|        200|Philips|2002|             200|     M|  8000|
|        300|   John|2010|             100|      |  6000|
+-----------+-------+----+----------------+------+------+

# Create another DataFrame:
employee_data1 =[(300,"John","2010","100","",6000),
                 (400,"Nancy","2001","400","F",1000),
                 (500,"Rosy","2014","500","M",5000)
               ]
employee_schema1 = ["employee_id","name","doj","employee_dept_id","gender","salary"]
df2 = spark.createDataFrame(data=employee_data1, schema = employee_schema1)
display(df2)

+-----------+-----+----+----------------+------+------+
|employee_id| name| doj|employee_dept_id|gender|salary|
+-----------+-----+----+----------------+------+------+
|        300| John|2010|             100|      |  6000|
|        400|Nancy|2001|             400|     F|  1000|
|        500| Rosy|2014|             500|     M|  5000|
+-----------+-----+----+----------------+------+------+

#Union Function
---------------
union = df1.union(df2)
display(union)

+-----------+-------+----+----------------+------+------+
|employee_id|   name| doj|employee_dept_id|gender|salary|
+-----------+-------+----+----------------+------+------+
|        100|Stephen|1999|             100|     M|  2000|
|        200|Philips|2002|             200|     M|  8000|
|        300|   John|2010|             100|      |  6000|
|        300|   John|2010|             100|      |  6000|
|        400|  Nancy|2001|             400|     F|  1000|
|        500|   Rosy|2014|             500|     M|  5000|
+-----------+-------+----+----------------+------+------+

# How to drop the Duplicate values in the union function: dropDuplicates()
--------------------------------------------------------------------------
union = df1.union(df2)
display(union.dropDuplicates())


## Pivot and Un-pivot functions
-------------------------------
# PIVOT FUNCTION EXAMPLE
------------------------
data = [("ABC","Q1",2000),
        ("ABC","Q2",3000),
        ("ABC","Q3",6000),
        ("ABC","Q4",1000),
        ("XYZ","Q1",5000),
        ("XYZ","Q2",4000),
        ("XYZ","Q3",1000),
        ("XYZ","Q4",2000),
        ("KLM","Q1",2000),
        ("KLM","Q2",3000),
        ("KLM","Q3",1000),
        ("KLM","Q4",5000)
         ]
column = ["Company","Qauter","Revenue"]
DF = spark.createDataFrame(data = data, schema = column).show()
+-------+------+-------+
|Company|Qauter|Revenue|
+-------+------+-------+
|    ABC|    Q1|   2000|
|    ABC|    Q2|   3000|
|    ABC|    Q3|   6000|
|    ABC|    Q4|   1000|
|    XYZ|    Q1|   5000|
|    XYZ|    Q2|   4000|
|    XYZ|    Q3|   1000|
|    XYZ|    Q4|   2000|
|    KLM|    Q1|   2000|
|    KLM|    Q2|   3000|
|    KLM|    Q3|   1000|
|    KLM|    Q4|   5000|
+-------+------+-------+

#PIVOT FUNCTION :
pivot = DF.groupBy("Company").pivot("Qauter").sum("Revenue")
display(pivot)

+-------+----+----+----+----+
|Company|  Q1|  Q2|  Q3|  Q4|
+-------+----+----+----+----+
|    KLM|2000|3000|1000|5000|
|    XYZ|5000|4000|1000|2000|
|    ABC|2000|3000|6000|1000|
+-------+----+----+----+----+

#UN-PIVOT FUNCTION :
un_pivot = pivot.selectExpr("Company", "stack(4,'Q1',Q1,'Q2',Q2,'Q3',Q3,'Q4',Q4) as (Quater,Revenue)")
display(un_pivot)
+-------+------+-------+
|Company|Qauter|Revenue|
+-------+------+-------+
|    KLM|    Q1|   2000|
|    KLM|    Q2|   3000|
|    KLM|    Q3|   1000|
|    KLM|    Q4|   5000|
|    XYZ|    Q1|   5000|
|    XYZ|    Q2|   4000|
|    XYZ|    Q3|   1000|
|    XYZ|    Q4|   2000|
|    ABC|    Q1|   2000|
|    ABC|    Q2|   3000|
|    ABC|    Q3|   6000|
|    ABC|    Q4|   1000|
+-------+------+-------+

# HANDLING CORRUPTED RECORD 
1. We need to create the schema first
    -> we have 3 types to handle the corruption  
         1. Perissive mode 
         2. DropMalformed Mode 
         3. FailFast mode 
         
Permissive mode: will keep the records and shows the corrupt records 
        ->df = spark.read.format("csv).option("mode", "PERMISSIVE").option("header", True).schema(schema_name).load("file location")
        -> display(df)
        
DropMalformed mode: this function will ignore the corrupted records 
         -> df = spark.read.format("csv").option("mode", "DROPMALFORMED").option("header", True).schema(schema_name).load("file_location")
         -> display(df)

FailFast Mode : this function will not execute in case any corrupt records are found 
        -> df = spark.read.format("csv").option("mode", "FAILFAST").option("header". True).schema(schema_name).load("file_location")
        
# To check the default Parallelism
----------------------------------
-> sc.defaultParallelism
     -> o/p 8 by default 
     
# How to check the partition Bytes by default 
---------------------------------------------
->spark.conf.get("spark.sql.files.maxPartitionBytes")
      -> by default 128 MB
   
#Getting data with in the spark environment 
-------------------------------------------
 from pyspark.sql.types import IntegerType 
 df = spark.createDataFrame(range(10), IntegerType())
 df.rdd.getNumPartitions()
      -> Out[18]: 8
      
#HOW TO KNOW THE EXECUTORS HAS BEEN PARTITIONS BY DEFAULT VALUE 
----------------------------------------------------------------
df.rdd.glom().collect()

Out[21]: [[Row(value=0)],
 [Row(value=1)],
 [Row(value=2)],
 [Row(value=3), Row(value=4)],
 [Row(value=5)],
 [Row(value=6)],
 [Row(value=7)],
 [Row(value=8), Row(value=9)]]
 
#HOW TO PARTITION A LARGE FILE WITH EXAMPLE
-------------------------------------------
df = spark.read.format("csv").option("Header", True).option("Inferschema", True).load("dbfs:/FileStore/tables/fifa_cleaned.csv")
#df.rdd.getNumPartitions()
#df.rdd.glom().collect()
#spark.conf.get("spark.sql.files.maxPartitionBytes")
#spark.conf.set("spark.sql.files.maxPartitionBytes", 100)

#REPARTITION:
-------------
repartition is used to increase or decreased the values of the parallelism.
-> df.rdd.getNumPartitions()
    -> By default the value is 8 
-> df.rdd.repartition(5)
    -> Out[7]: 5
    
# COALESCE:
------------
this function is only used to reduce the partitions 
 #How to get the partition number ?
 ----------------------------------
    -> df.rdd.getNumPartitions()
            -> By default the value is 8 
            
 #How to resize the partition number?
 ------------------------------------
    -> df.rdd.repartition(3)
        -> df.rdd.getNumPartitions()
             -> Now the value is 3 
    -> df.rdd.glom().collect()
